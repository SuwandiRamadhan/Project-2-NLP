{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1da461",
   "metadata": {},
   "source": [
    "# Data & Algoritma Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28d2c3",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac478f9",
   "metadata": {},
   "source": [
    "### ðŸ“Š Nama Dataset\n",
    "liputan6_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c5914",
   "metadata": {},
   "source": [
    "### ðŸŒ Languages\n",
    "- Indonesian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f459f8",
   "metadata": {},
   "source": [
    "### ðŸ§© Data Structure\n",
    "|Nama Kolom|Tipe Data|\n",
    "|----|--------|\n",
    "|`id`|`string`|\n",
    "|`url`|`string`|\n",
    "|`clean_article`|`string`|\n",
    "|`clean_summary`|`string`|\n",
    "|`extractive_summary`|`string`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00beada",
   "metadata": {},
   "source": [
    "### Data Instances\n",
    "|Nama Kolom|Contoh Data|\n",
    "|----------|-----------|\n",
    "|`id`|26408|\n",
    "|`url`|https://www.liputan6.com/news/read/26408/pbb-siap-membantu-penyelesaian-konflik-ambon|\n",
    "|`clean_article`|Liputan6.com, Ambon: Partai Bulan Bintang wilayah Maluku bertekad membantu pemerintah menyelesaikan konflik di provinsi tersebut. Syaratnya, penanganan penyelesaian konflik Maluku harus dimulai dari awal kerusuhan, yakni 19 Januari 1999. Demikian hasil Musyawarah Wilayah I PBB Maluku yang dimulai Sabtu pekan silam dan berakhir Senin (31/12) di Ambon. Menurut seorang fungsionaris PBB Ridwan Hasan, persoalan di Maluku bisa selesai asalkan pemerintah dan aparat keamanan serius menangani setiap persoalan di Maluku secara komprehensif dan bijaksana. Itulah sebabnya, PBB wilayah Maluku akan menjadikan penyelesaian konflik sebagai agenda utama partai. PBB Maluku juga akan mendukung penegakan hukum secara terpadu dan tanpa pandang bulu. Siapa saja yang melanggar hukum harus ditindak. Ridwan berharap, Ketua PBB Maluku yang baru, Ali Fauzi, dapat menindak lanjuti agenda politik partai yang telah diamanatkan dan mau mendukung penegakan hukum di Maluku. (ULF/Sahlan Heluth).|\n",
    "|`clean_summary`|Konflik Ambon telah berlangsung selama tiga tahun. Partai Bulan Bintang wilayah Maluku siap membantu pemerintah menyelesaikan kasus di provinsi tersebut.|\n",
    "|`extractive_summary`|Liputan6.com, Ambon: Partai Bulan Bintang wilayah Maluku bertekad membantu pemerintah menyelesaikan konflik di provinsi tersebut. Siapa saja yang melanggar hukum harus ditindak.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acce5cb",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "|Nama Kolom|Keterangan|\n",
    "|----------|----------|\n",
    "|`id`|Kolom id unique|\n",
    "|`url`|URL Article|\n",
    "|`clean_article`|Isi original article|\n",
    "|`clean_summary`|Ringkasan Abstract|\n",
    "|`extractive_summary`|Ringkasan Ekstractif|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a2107",
   "metadata": {},
   "source": [
    "## Algoritma Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525b862",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2557920",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28817c3e",
   "metadata": {},
   "source": [
    "### Load Dataset dan Convert Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28cb5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3798e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = glob.glob('../data/liputan6_data/canonical/train/*.json')\n",
    "# df_list = [pd.read_json(f, lines=True) for f in file_list]\n",
    "# df_train = pd.concat(df_list, ignore_index=True)\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecb91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('../data/train_data.csv', index=False)\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c057e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = glob.glob('../data/liputan6_data/canonical/dev/*.json')\n",
    "# df_list = [pd.read_json(f, lines=True) for f in file_list]\n",
    "# df_dev = pd.concat(df_list, ignore_index=True)\n",
    "# df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c99d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev.to_csv('../data/dev_data.csv', index=False)\n",
    "# df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65644f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = glob.glob('../data/liputan6_data/canonical/test/*.json')\n",
    "# df_list = [pd.read_json(f, lines=True) for f in file_list]\n",
    "# df_test = pd.concat(df_list, ignore_index=True)\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0641fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.to_csv('../data/test_data.csv', index=False)\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d917f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('../data/train_data.csv')\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280f58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev = pd.read_csv('../data/dev_data.csv')\n",
    "# df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4e3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv('../data/test_data.csv')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6abbca",
   "metadata": {},
   "source": [
    "## Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53198aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf73b57",
   "metadata": {},
   "source": [
    "### Memuat Data dan Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat data...\n",
      "Membersihkan data...\n",
      "Membagi data menjadi set Training (60%), Validasi (20%), dan Testing (20%)...\n",
      "Jumlah artikel untuk Training: 6582\n",
      "Jumlah artikel untuk Validasi: 2195\n",
      "Jumlah artikel untuk Testing: 2195\n",
      "\n",
      "Mengubah data menjadi format per-kalimat...\n",
      "Jumlah kalimat di data Training: 76399\n",
      "Jumlah kalimat di data Validasi: 25819\n",
      "Jumlah kalimat di data Testing: 26240\n"
     ]
    }
   ],
   "source": [
    "def parse_list_string(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Memuat dataset\n",
    "print(\"Memuat data...\")\n",
    "df_all = pd.read_csv('../data/train2_data.csv')\n",
    "\n",
    "# Membersihkan data\n",
    "print(\"Membersihkan data...\")\n",
    "df_all['clean_article'] = df_all['clean_article'].apply(parse_list_string)\n",
    "df_all['extractive_summary'] = df_all['extractive_summary'].apply(parse_list_string)\n",
    "\n",
    "\n",
    "print(\"Membagi data menjadi set Training (60%), Validasi (20%), dan Testing (20%)...\")\n",
    "\n",
    "df_train_val, df_test_articles = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_train_articles, df_val_articles = train_test_split(\n",
    "    df_train_val,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Jumlah artikel untuk Training: {len(df_train_articles)}\")\n",
    "print(f\"Jumlah artikel untuk Validasi: {len(df_val_articles)}\")\n",
    "print(f\"Jumlah artikel untuk Testing: {len(df_test_articles)}\")\n",
    "\n",
    "\n",
    "# Fungsi untuk mengubah format dari per-artikel menjadi per-kalimat\n",
    "def create_sentence_dataframe(df):\n",
    "    processed_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_sentences_tokens = row['clean_article']\n",
    "        summary_indices = set(row['extractive_summary'])\n",
    "        sentences_as_strings = [\" \".join(sent) for sent in article_sentences_tokens]\n",
    "        for i, sent_text in enumerate(sentences_as_strings):\n",
    "            processed_data.append({\n",
    "                'sentence': sent_text,\n",
    "                'label': 1 if i in summary_indices else 0,\n",
    "            })\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Buat DataFrame per-kalimat untuk setiap set secara terpisah\n",
    "print(\"\\nMengubah data menjadi format per-kalimat...\")\n",
    "train_sentences_df = create_sentence_dataframe(df_train_articles)\n",
    "val_sentences_df = create_sentence_dataframe(df_val_articles)\n",
    "test_sentences_df = create_sentence_dataframe(df_test_articles)\n",
    "\n",
    "print(f\"Jumlah kalimat di data Training: {len(train_sentences_df)}\")\n",
    "print(f\"Jumlah kalimat di data Validasi: {len(val_sentences_df)}\")\n",
    "print(f\"Jumlah kalimat di data Testing: {len(test_sentences_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd9a94",
   "metadata": {},
   "source": [
    "### Tokenisasi dengan IndoBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8086cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memuat tokenizer dari indobenchmark/indobert-base-p1...\n",
      "\n",
      "Melakukan tokenisasi pada semua dataset...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
    "MAX_LEN = 128\n",
    "\n",
    "print(f\"\\nMemuat tokenizer dari {MODEL_NAME}...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def convert_to_bert_input(sentences, tokenizer, max_len):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        sentences.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "print(\"\\nMelakukan tokenisasi pada semua dataset...\")\n",
    "# DIUBAH: Tokenisasi semua set data secara terpisah\n",
    "X_train_tokens = convert_to_bert_input(train_sentences_df['sentence'], tokenizer, MAX_LEN)\n",
    "X_val_tokens = convert_to_bert_input(val_sentences_df['sentence'], tokenizer, MAX_LEN)\n",
    "X_test_tokens = convert_to_bert_input(test_sentences_df['sentence'], tokenizer, MAX_LEN)\n",
    "\n",
    "# DIUBAH: Ambil label untuk setiap set secara terpisah\n",
    "y_train = train_sentences_df['label'].values\n",
    "y_val = val_sentences_df['label'].values\n",
    "y_test = test_sentences_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6f9a5",
   "metadata": {},
   "source": [
    "### MEMBANGUN DAN MELATIH MODEL IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68a92dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memuat pre-trained IndoBERT model...\n",
      "WARNING:tensorflow:From C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p1 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1244413   ['input_ids[0][0]',           \n",
      " )                           ngAndCrossAttentions(last_   44         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tf_bert_model[0][0]']       \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 768)                  0         ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    769       ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124442113 (474.71 MB)\n",
      "Trainable params: 124442113 (474.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Memulai training model...\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "1194/1194 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.8190 - precision: 0.5010 - recall: 0.1410 \n",
      "Epoch 1: val_loss improved from inf to 0.42518, saving model to indobert_summarizer_best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194/1194 [==============================] - 13332s 11s/step - loss: 0.4534 - accuracy: 0.8190 - precision: 0.5010 - recall: 0.1410 - val_loss: 0.4252 - val_accuracy: 0.8294 - val_precision: 0.5525 - val_recall: 0.2186\n",
      "Epoch 2/2\n",
      "1194/1194 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8294 - precision: 0.5950 - recall: 0.1825 \n",
      "Epoch 2: val_loss improved from 0.42518 to 0.42249, saving model to indobert_summarizer_best_model.h5\n",
      "1194/1194 [==============================] - 14363s 12s/step - loss: 0.4150 - accuracy: 0.8294 - precision: 0.5950 - recall: 0.1825 - val_loss: 0.4225 - val_accuracy: 0.8299 - val_precision: 0.5812 - val_recall: 0.1588\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Training selesai.\n"
     ]
    }
   ],
   "source": [
    "def build_model(bert_model, max_len=128):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    sequence_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    cls_token_output = sequence_output[:, 0, :]\n",
    "    x = tf.keras.layers.Dropout(0.2)(cls_token_output)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "print(\"\\nMemuat pre-trained IndoBERT model...\")\n",
    "bert_model = TFBertModel.from_pretrained(MODEL_NAME)\n",
    "model = build_model(bert_model, max_len=MAX_LEN)\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy('accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='indobert_summarizer_best_model.h5',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2 \n",
    "\n",
    "print(\"\\nMemulai training model...\")\n",
    "\n",
    "# Konversi input menjadi format dictionary yang bisa dibaca model\n",
    "train_input_dict = {'input_ids': tf.constant(X_train_tokens['input_ids']), 'attention_mask': tf.constant(X_train_tokens['attention_mask'])}\n",
    "val_input_dict = {'input_ids': tf.constant(X_val_tokens['input_ids']), 'attention_mask': tf.constant(X_val_tokens['attention_mask'])}\n",
    "\n",
    "history = model.fit(\n",
    "    train_input_dict,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_input_dict, y_val),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "print(\"Training selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14406d93",
   "metadata": {},
   "source": [
    "### Evaluasi Model pada Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memuat model terbaik dari checkpoint untuk evaluasi...\n",
      "\n",
      "Melakukan evaluasi pada Test Set (data yang belum pernah dilihat)...\n",
      "820/820 [==============================] - 1294s 2s/step\n",
      "\n",
      "Classification Report (Test Set):\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Bukan Ringkasan (0)       0.84      0.98      0.91     21610\n",
      "      Ringkasan (1)       0.59      0.16      0.25      4630\n",
      "\n",
      "           accuracy                           0.83     26240\n",
      "          macro avg       0.72      0.57      0.58     26240\n",
      "       weighted avg       0.80      0.83      0.79     26240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMemuat model terbaik dari checkpoint untuk evaluasi...\")\n",
    "\n",
    "best_model = tf.keras.models.load_model(\n",
    "    'indobert_summarizer_best_model.h5', \n",
    "    custom_objects={\"TFBertModel\": TFBertModel}\n",
    ")\n",
    "\n",
    "# Siapkan data test\n",
    "test_input_dict = {'input_ids': tf.constant(X_test_tokens['input_ids']), 'attention_mask': tf.constant(X_test_tokens['attention_mask'])}\n",
    "\n",
    "print(\"\\nMelakukan evaluasi pada Test Set (data yang belum pernah dilihat)...\")\n",
    "y_pred_proba = best_model.predict(test_input_dict)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Bukan Ringkasan (0)', 'Ringkasan (1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e3df4",
   "metadata": {},
   "source": [
    "### INFERENCE (MEMBUAT RINGKASAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5688f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Membuat Ringkasan untuk Artikel Sampel dari Test Set ---\n",
      "\n",
      "Artikel Asli (5 kalimat pertama):\n",
      "- Liputan6 . com , Jakarta : Pengamat politik Andi Malarangeng , baru-baru ini , menilai pembaharuan kode etik DPR tidak akan berarti banyak dalam mengubah etika berpolitik para anggota Dewan .\n",
      "- Sebab , pengaturan masalah etika belum ditetapkan dalam Undang-undang .\n",
      "- Menurut Andi , muatan kode etik DPR yang akan disampaikan pekan ini masih terlihat samar .\n",
      "- Selain itu , hubungan antara Pasal Satu dan lainnya tak memiliki konsekuensi yang jelas .\n",
      "- Akibatnya , kode etik tersebut hanya akan menimbulkan perdebatan .\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "\n",
      "Ringkasan yang Dihasilkan Model:\n",
      "Liputan6 . com , Jakarta : Pengamat politik Andi Malarangeng , baru-baru ini , menilai pembaharuan kode etik DPR tidak akan berarti banyak dalam mengubah etika berpolitik para anggota Dewan . Sebab , para anggota DPR juga terkait dengan kode etik partai . Rencananya , awal Oktober ini , DPR akan membahas pembaharuan kode etik tersebut .\n"
     ]
    }
   ],
   "source": [
    "def summarize_article(article_sentences, model, tokenizer, max_len, num_sentences=3):\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        article_sentences,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    input_dict = {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']}\n",
    "    probabilities = model.predict(input_dict).flatten()\n",
    "    top_indices = np.argsort(probabilities)[-num_sentences:]\n",
    "    top_indices = sorted(top_indices)\n",
    "    summary = \" \".join([article_sentences[i] for i in top_indices])\n",
    "    return summary\n",
    "\n",
    "\n",
    "sample_article_data = df_test_articles['clean_article'].iloc[0]\n",
    "sample_sentences = [\" \".join(sent) for sent in sample_article_data]\n",
    "\n",
    "print(f\"\\n--- Membuat Ringkasan untuk Artikel Sampel dari Test Set ---\")\n",
    "print(\"\\nArtikel Asli (5 kalimat pertama):\")\n",
    "for sent in sample_sentences[:5]:\n",
    "    print(\"- \" + sent)\n",
    "\n",
    "\n",
    "generated_summary = summarize_article(sample_sentences, best_model, tokenizer, max_len=MAX_LEN, num_sentences=3)\n",
    "\n",
    "print(\"\\nRingkasan yang Dihasilkan Model:\")\n",
    "print(generated_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
